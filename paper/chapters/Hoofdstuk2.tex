\graphicspath{{images/chapter2/}}

\chapter{Style Transfer Model Selection and Building}
\label{chap:style_transfer}

The goal is to improve pose estimation on art collections.
For this effort, two methods will be investigated: 
First, the input to existing models will be transformed from artworks to photographs.
Second, the models will be fine-tuned on an COCO dataset which is augmented with synthetic COCO images.
Both these methods need a style transfer model that is trained to do a transformation between an art movement and realistic images.
Therefor, three algorithms for style transfer will be explored.
The motivation for the choices of the algorithms will be explained in full detail.
The different models will be trained on three datasets of different art movements and evaluated based on different metrics.
There are several considerations to be made when choosing the right model.
The focus will mainly be on methods that have code readily available.
At the same time, there should be a wide variation in architectures.
It makes little sense to analyze two similar architectures here, as the improvements between them has already been well documented in their corresponding papers and is not threading new ground.
All these criteria are considered in the next sections as well as those uniquely for each section.

\section{Training Style Transfer}
\label{sec:baseline_style_transfer}

\subsection{Choice of Model}
\label{sec:baseline_choice_style_transfer}
The most important criteria for style transfer is the quality.
For the baseline, the photographs need to be inseparable from any artworks for the measurements to be useful.
Pose estimation is trained on photographs, so style transfer needs to create accurate photographs.
However, measuring the quality of an image is a difficult task.
Numerous metrics each based on different criteria exist due to the absence of a universally agreed-upon metric \cite{ioannou2024}, but there is a general consensus that it should closely resemble human evaluation.
Of all the different models, the younger models aim more on finding a transformation mapping rather than merely doing texture transfer.
To keep the complexity low, the focus will only be on the latter, while keeping to the main advancements.
As previously mentioned, a diverse range of architectures should be selected.
For these reasons, \gls{AdaIN} \cite{Huang2017} was selected from the feed-forward generation networks.
It's also one of the networks which can transform from an arbitrary style unlike the other selected networks.
CycleGAN \cite{Zhu2017b} is a major breakthrough in the training scheme of \glspl{GAN} and cycle-consistency loss has since been incorporated in most new models.
It also has several pre-trained networks in the styles of several important artists, which makes it an easy second choice.
Another interesting concept is that of latent space, where the assumption is that there exists a common space that can encode information from several domains \cite{Liu2017}.
This is used in StarGANv2 \cite{Choi2019} to implement a model that can transform images between several different domains using the same network.
In the future, StarGANv2 will be described as StarGAN.
Each of these models represent a significant contribution to the field of image-to-image translation and will be analyzed thusly.
\\

\subsection{Creation of datasets}
\label{sec:baseline_dataset_style_transfer}
None of the selected models has any pre-trained weights for certain art movements, so new models need to be made.
The most popular and most used dataset for this seems to be the WikiArt dataset.
It categorizes the artworks into several art movements, but also multiple genres as summarized in Table \ref{tab:wikiart_genres_and_styles}.
To keep complexity low, the transformation between styles should be as small as possible, which eliminates the abstract styles as a potential choice, however they should not be hyper realistic either as then they would be so similar to photographs that the benefits of the improvements are meaningless.
There are plenty of styles that are compatible with these criteria and also have plenty of images to create a well sized subset.
The choice of style beyond that point is completely the result of the bias of the author.
This results of the selection being: Baroque, Renaissance and Impressionism.

\begin{table}[h]
    \setlength\tabcolsep{4pt}
    \vspace{0.2em}
    \caption{List of the selected genres and names of the styles in the WikiArt dataset. \cite{Saleh2015} }
    \centering
    \footnotesize
    \label{tab:wikiart_genres_and_styles}
    \begin{tabular}{ c|m{0.8\textwidth} }
        \hline
        \bf{Task Name}&\bf{List of Members}\cr
        \hline
        Genre & abstract painting, cityscape, genre painting, illustration, landscape, nude painting, portrait, religious painting, sketch and study, still life \cr
        \hline
        Style & Abstract Expressionism, Action Painting, Analytical Cubism, Art Nouveau-Modern Art, Baroque, Color Field Painting, Contemporary Realism, Cubism, Early Renaissance, Expressionism, Fauvism, High Renaissance, Impressionism, Mannerism- Late-renaissance, Minimalism, Primitivism- Naive Art, New Realism, Northern Renaissance, Pointillism, Pop Art, Post Impressionism, Realism, Rococo, Romanticism, Symbolism, Synthetic Cubism, Ukiyo-e \cr
        \hline 
    \end{tabular}
\end{table}

The impressionist style is chosen because it is more colorful and abstract than the others.
Baroque and Renaissance are both very dark and very similar in style, but renaissance artworks are just a bit more stylized.
This was a deliberate choice to see if there's possibly a difference between these attributes.
The Cezanne2photo dataset \cite{Zhu2017b} was looked at to get an idea of what an adequate sized dataset should be.
The conclusion is that it should at least be above 500 images.
A bigger dataset is better, but there are only so many artworks available.
In the end, the size for all except one are around 800 images.
More details about this can be found in the Figures \ref{fig:photograph_style_transfer_dataset}, \ref{fig:baroque_style_transfer_dataset}, \ref{fig:renaissance_style_transfer_dataset}, and \ref{fig:impressionism_style_transfer_dataset}.
When looking at the datasets mainly used by the unsupervised image-to-image models, there is a very specific focus on certain domains.
\gls{AFHQ} used for StarGANv2 or Horse$\leftrightarrow$Zebra show that the training images put the subject central in the image.
This means that for each art movement, a subset needs to be created with images that contain full body poses as well as crowded images, as this is what the pose estimation models are trained on.
While there's a high variation of genres in the WikiArt dataset, they do not adequately subdivide the dataset for this problem.
At first glance, it seems that the genres "nude painting" and "portrait" would give a good set of images to use, however there are still multiple problems.
The portraits are mostly zoomed in from the chest up (Figure \ref{fig:wikiart_shortcomings_portraits}).
There should be a higher variation in poses than that.
Like with the nude paintings, but those don't have as many images to create a dataset from (Figure \ref{fig:wikiart_shortcomings_nudes}).
Another genre that might be promising, is "genre painting", but those don't always have the model central to the image (Figure \ref{fig:wikiart_shortcomings_genre}).
Overall, there is still a high variety of style even within the different art movements.
There is also the presence of sketches or graphite drawings (Figure \ref{fig:wikiart_shortcomings_style}).
As discussed previously, the art movements were deliberately chosen to see if certain attributes, e.g. color and abstraction, have an influence on the performance of style transfer.
It is important then to have a consistent style in each dataset which is not possible to create by just splitting the genres provided by WikiArt.
To achieve this, an algorithm was sought to find similar images.

\textbf{Feature extraction}
First, an algorithm that extracts features using VGG16-features from the images was used \cite{Roman2023}.
It calculates the cosine distance between the image features, and groups them using DBSCAN \cite{Ester1996}.
This did not yield any promising results.
Instead of VGG16, YOLOv8 \cite{Jocher_Ultralytics_YOLO_2023} was substituted for feature extraction, but this also didn't provide satisfactory results.

\textbf{Content Based Image Retrieval}
Another way to find similar images is with \gls{CBIR}.
Using a query image it can find similar looking images.
Because this algorithm is trained to recognize similar instances and not a specific style or genre, the query image needs to be carefully selected.
When there's another recognizable instance besides a person in the image it will also score images with that instance highly.
Figure \ref{fig:cbir_failed_query} shows how a car or a flower pattern is enough to find different instances.
On the other hand, some activities are so distinct that only instances of that activity are found, like tennis.

A photograph dataset is also needed to do proper training, therefore the same procedure is also applied to the COCO dataset to create a human central COCO subset.
The figures \ref{fig:photograph_style_transfer_dataset}, \ref{fig:baroque_style_transfer_dataset}, \ref{fig:impressionism_style_transfer_dataset} and \ref{fig:renaissance_style_transfer_dataset} shows the query images used to construct the different datasets along with a selection of the dataset.

\begin{figure}
	\centering
    \includegraphics[height=2in]{edouard-manet_berthe-morisot-with-a-bouquet-of-violets-1872}
    \hspace{0.1in}
    \includegraphics[height=2in]{johannes-vermeer_the-girl-with-a-pearl-earring}
    \hspace{0.1in}
    \includegraphics[height=2in]{raphael_portrait-of-the-young-pietro-bembo-1504}
	\caption{
        Portraits are mainly from the chest up.
	}
    \label{fig:wikiart_shortcomings_portraits}
\end{figure}
\begin{figure}
    \centering
    \captionsetup{justification=centering}
    \includegraphics[height=2in]{artemisia-gentileschi_lucretia-1620}
    \hspace{0.1in}
    \includegraphics[height=2in]{michelangelo_ignudo-16}
    \hspace{0.1in}
    \includegraphics[height=2in]{pierre-auguste-renoir_bathers-1917}
	\caption{
        Nudes have a better variation of poses, but a small number of images. \\
        23 for baroque, 247 for impressionism and 21 for renaissance.
	}
    \label{fig:wikiart_shortcomings_nudes}
\end{figure}
\begin{figure}
    \centering
    {
        \includegraphics[height=1.6in]{edgar-degas_dancers-at-the-old-opera-house}%
        \hspace{0.1in}
        \includegraphics[height=1.6in]{jan-siberechts_the-market-garden-1664}%
        \hspace{0.1in}
        \includegraphics[height=1.6in]{raphael_the-fire-in-the-borgo-1514}%
        \vspace{0.1in}
    }
    {%
        \includegraphics[height=1.6in]{rembrandt_a-man-in-a-room-1630}%
        \hspace{0.1in}
        \includegraphics[height=1.6in]{vittore-carpaccio_hunting-on-the-lagoon}%
        \hspace{0.1in}
        \includegraphics[height=1.6in]{william-merritt-chase_an-early-stroll-in-the-park}%
    }
	\caption{
        In genre paintings, humans are less central to the painting.
	}
    \label{fig:wikiart_shortcomings_genre}
\end{figure}
\begin{figure}
    \centering
    {
        \includegraphics[height=1.6in]{adriaen-van-de-venne_a-man-carrying-a-sack}%
        \hspace{0.1in}
        \includegraphics[height=1.6in]{andrea-mantegna_trumpeters-carrying-flags-and-banners-1500}%
        \hspace{0.1in}
        \includegraphics[height=1.6in]{auguste-rodin_untitled(5)}%
        \vspace{0.1in}
    }
    {%
        \includegraphics[height=1.6in]{berthe-morisot_at-the-exposition-palace}%
        \hspace{0.1in}
        \includegraphics[height=1.6in]{claude-lorrain_harbour-scene}%
        \hspace{0.1in}
        \includegraphics[height=1.6in]{rembrandt_seated-naked-woman-woman-bathing-her-feet-at-a-brook-1658}%
    }
    \caption{
        The variation in style within the different art movements.
    }
    \label{fig:wikiart_shortcomings_style}
\end{figure}
\begin{figure}
    \centering
	\subcaptionbox{The flower pattern is isolated}{%
        \includegraphics[height=1.5in]{000000001988}%
        \hspace{0.1in}
        \includegraphics[height=1.25in]{21_2_000000299309}%
        \includegraphics[height=1.25in]{22_1_000000090429}%
        \includegraphics[height=1.25in]{22_2_000000396743}%
        \includegraphics[height=1.25in]{25_1_000000376731}%
    }
	\subcaptionbox{The car and concrete are isolated}{%
        \includegraphics[height=1.5in]{99_1_000000428718}%
        \hspace{0.1in}
        \includegraphics[height=0.85in]{22_1_000000402051}%
        \includegraphics[height=0.85in]{22_4_000000557686}%
        \includegraphics[height=0.85in]{22_5_000000547246}%
        \includegraphics[height=0.85in]{26_1_000000466981}%
    }
    \caption{
        Examples of failed queries for CBIR. The left image is the query image.
    }
    \label{fig:cbir_failed_query}
\end{figure}
\begin{figure}
	\centering
    {
        \includegraphics[height=2in]{100_1_000000122135 - kopie}
        \includegraphics[height=2in]{100_1_000000352073}\\
    }
	\subcaptionbox{Query images}{%
        \includegraphics[height=2in]{100_1_000000545793}
        \includegraphics[height=2in]{99_1}
	}
    {
        \\
        \includegraphics[height=2in]{22_38_000000036051}
        \includegraphics[height=2in]{28_1_000000032850}
    }
	\subcaptionbox{Resulting dataset}{
        \includegraphics[height=2in]{22_21_000000084015 - kopie}
        \includegraphics[height=2in]{21_84_000000409291}
        \includegraphics[height=2in]{22_6_000000242745}
	}
	\caption{The photograph dataset consists of 825 images.}
	\label{fig:photograph_style_transfer_dataset}
\end{figure}
\begin{figure}
	\centering
	\subcaptionbox{Query images}{
        \includegraphics[height=2in]{99_1_alonzo-cano_st-john-the-evangelist-and-the-poisoned-cup}
        \includegraphics[height=2in]{100_1_peter-paul-rubens_venus-cupid-bacchus-and-ceres-1613}
	}
	{
        \\
        \includegraphics[height=2in]{22_12_jusepe-de-ribera_st-john-the-baptist-in-the-wilderness}
        \includegraphics[height=2in]{24_10_alonzo-cano_the-virgin-and-child-1643}\\
    }
    {
        \includegraphics[height=2in]{21_11_le-nain-brothers_the-family-meal-1642}
        \includegraphics[height=2in]{15_96_peter-paul-rubens_st-sebastian}\\
    }
	\subcaptionbox{Resulting dataset}{
        \includegraphics[height=2in]{32_3_peter-paul-rubens_the-three-graces}
    }
	\caption{The baroque dataset consists of 518 images.}
	\label{fig:baroque_style_transfer_dataset}
\end{figure}
\begin{figure}
	\centering
	\subcaptionbox{Query images}{
        \includegraphics[height=2in]{100_1_valentin-serov_portrait-of-maria-akimova-1908 - kopie}
        \includegraphics[height=2in]{99_1_pierre-auguste-renoir_the-large-bathers-1887}
	}
	{
        \includegraphics[height=2in]{30_18_pierre-auguste-renoir_the-judgment-of-paris-1914}
        \includegraphics[height=2in]{36_65_federico-zandomeneghi_the-reader}
    }
    {
        \includegraphics[height=2in]{23_44_edgar-degas_racehorses-at-longchamp-1875 - kopie}
        \includegraphics[height=2in]{29_13_henri-martin_young-women-in-garden-in-marquayrol}
    }
    \subcaptionbox{Resulting dataset}{
        \includegraphics[height=2in]{37_79_edgar-degas_jeantaud-linet-and-laine-1871}
	}
	\caption{The impressionism dataset consists of 780 images.}
	\label{fig:impressionism_style_transfer_dataset}
\end{figure}
\begin{figure}
	\centering
	\subcaptionbox{Query images}{
        \includegraphics[height=2in]{100_1_titian_venus-anadyomene}
        \includegraphics[height=2in]{100_1_raphael_madonna-of-the-goldfinch}
    }
    {
        \\
        \includegraphics[height=2in]{16_10_leonardo-da-vinci_the-lady-with-the-ermine-cecilia-gallerani-1496}
        \includegraphics[height=2in]{19_4_pieter-bruegel-the-elder_peasant-and-birdnester-1568}
    }
    {
        \\
        \includegraphics[height=2in]{26_4_raphael_the-three-graces-1505}
        \includegraphics[height=2in]{33_1_raphael_the-deposition-1507}
        \\
	}
	\subcaptionbox{Resulting dataset}{%
        \includegraphics[height=2in]{25_2_pietro-perugino_madonna-with-child}
	}
	\caption{The renaissance dataset consists of 790 images.}
	\label{fig:renaissance_style_transfer_dataset}
\end{figure}

\newpage
\subsection{Training}
\label{sec:baseline_training_style_transfer}
From the selected models there are only two that require training, CycleGAN and StarGAN.
AdaIN can use any arbitrary style from a content image to do style transfer.
This eliminates the need to train a new model for it and the pre-trained model can be used for the experiments.
The other models will be trained with the provided default parameters.
No hyperparameter tuning will be done as the goal is to measure the performance between different approaches and not optimize a single model.

\textbf{CycleGAN} was trained using a different number of epochs for each style to compare the performance .
Baroque was trained for 200 and 2000 epochs, renaissance for 500 epochs and impressionism for 750 epochs.

\textbf{StarGAN} does not use epochs to determine the training progression, or, at least, the pytorch implementation doesn't.
The model was trained to find a mapping between all different datasets for 100,000 iterations.

\newpage
\subsection{Results}
\label{sec:baseline_results_style_transfer}
\subsubsection{Qualitative Evaluation}
As shown in Figure \ref{fig:difference_AdaIN_CycleGAN}, AdaIN removes more of the details of the content than CycleGAN does, but as expected the style transfer is completely dependent on the style image used.
CycleGAN does look like it is able to capture the general style of the learned art movements, e.g. baroque and renaissance are dark, and impressionism is colorful.
StarGAN unfortunately experiences modal collapse.
In the examples, either the images become complete random splatter, or it is not able to find a correct mapping between the content of different images, e.g. in one image the face is mapped to the back.
Looking at the different epochs, it seems that after more epochs the stylization is stronger.
All in all, the results are very disappointing as none of the images look like they're a painting from a different time.

\begin{figure}[h]
	\centering
	{%
        \includegraphics[width=0.23\textwidth,height=1.5in]{000000564404}
        \includegraphics[width=0.23\textwidth,height=1.5in]{000000564404_adain_renaissance_c}
        \includegraphics[width=0.23\textwidth,height=1.5in]{000000564404_baroque_200}
        \includegraphics[width=0.23\textwidth,height=1.5in]{000000564404_stargan_impressionism}
	}
	{%
        \includegraphics[width=0.23\textwidth,height=1in]{000000566054}
        \includegraphics[width=0.23\textwidth,height=1in]{000000566054_adain_renaissance_c}
        \includegraphics[width=0.23\textwidth,height=1in]{000000566054_baroque_200}
        \includegraphics[width=0.23\textwidth,height=1in]{000000566054_stargan_impressionism}
	}
	{%
        \includegraphics[width=0.23\textwidth,height=2.2in]{000000568462}
        \includegraphics[width=0.23\textwidth,height=2.2in]{000000568462_adain_renaissance_a}
        \includegraphics[width=0.23\textwidth,height=2.2in]{000000568462_baroque_200}
        \includegraphics[width=0.23\textwidth,height=2.2in]{000000568462_stargan_impressionism}
	}
	\caption{Left is the content image. The middle-left is AdaIN using a renaissance style image. The middle-right is CycleGAN using the baroque style. The right is StarGAN impressionism. AdaIN abstracts the features more than CycleGAN, while StarGAN experiences modal collapse.}
	\label{fig:difference_AdaIN_CycleGAN}
\end{figure}

\begin{figure}[h]
	\centering
	{%
        \includegraphics[width=0.23\textwidth,height=2in]{000000000036}
        \includegraphics[width=0.23\textwidth,height=2in]{000000000036_adain_renaissance_a}
        \includegraphics[width=0.23\textwidth,height=2in]{000000000036_baroque_200}
        \includegraphics[width=0.23\textwidth,height=2in]{000000000036_stargan_impressionism}
    }
	\caption{Left is the content image. The middle-left is AdaIN using a renaissance style image. The middle-right is CycleGAN using the baroque style. The right is StarGAN impressionism. AdaIN abstracts the features more than CycleGAN, while StarGAN experiences modal collapse.}
	\label{fig:difference_AdaIN_CycleGAN_2}
\end{figure}

\subsubsection{Quantitative Evaluation}
To evaluate the trained models, there exist several metrics, which are discussed in section \ref{sec:style_transfer_metrics}.
Before applying the evaluation metrics there needs to be an adequate dataset to do meaningful measurements on first.
Two datasets are considered for this purpose:
\begin{enumerate}
    \item \gls{AST-IQAD} is a set specifically made to measure style transfer \cite{Chen2023}.
    It constructs the set around several inter-subjective characteristics and categories.
    This means that these criteria of subjective evaluation are mostly agreed upon across a group of people.
    Among those are: color tone, brush stroke, distribution of objects, and contents.
    While it also declares a set of style images, those will not be used.
    \item Since the content of the problem of this thesis only focuses around persons and the \gls{AST-IQAD} dataset works with different kinds of content, a custom dataset is created that focuses around persons.
    This is created the same way the style transfer datasets were created.
\end{enumerate}
For the evaluation, AdaIN cycles through the style images which it was trained on to use as input style images.
The perceptual distance needs a content and style image to be able to make an evaluation.
For AdaIN, it is clear what needs to be used here, but for the other models this metric seems useless.
However, the dataset that CycleGAN and StarGAN were trained on can be used as style images for this.
The style features of the generated images should still be similar as the ones it was trained on.
These same datasets are also used for the real image distribution needed for \gls{FID} and \gls{LPIPS}.

In Table \ref{tab:performance_style_transfer_by_dataset}, the results of the evaluation are available.
No model seems to distinct itself from the others.
In fact, a pattern arises where AdaIN clearly does well with \gls{PD} and {FID}, StarGAN does well with \gls{IS}, CycleGAN does not do well in any, and \gls{LPIPS} has similar results for all.
Impressionism does the best out of all of the styles.
Table \ref{tab:performance_style_transfer_by_dataset} shows the same results, but grouped by model.
This shows that the custom dataset has a slightly better evaluation.

\begin{table}[h]
    \setlength\tabcolsep{4pt}
    \vspace{0.2em}
    \caption{Performance comparison of Style Transfer measured by various metrics grouped by dataset; Perceptual Distance (PD), Inception score (IS), Fréchet Inception Distance (FID) and Learned Perceptual Image Patch Similarity (LPIPS).}
    \begin{center}
    \footnotesize
    \label{tab:performance_style_transfer_by_dataset}
    \begin{tabular}{ l|cccc|cccc|cccc }
        \hline
        \multirow{2}{*}{\bf{Method}}&\multicolumn{4}{c|}{\bf{Baroque}}&\multicolumn{4}{c|}{\bf{Impressionism}}&\multicolumn{4}{c}{\bf{Renaissance}}\cr
        &\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}&\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}&\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}\cr
        \hline
        \multicolumn{13}{c}{\bf{AST-IQAD Dataset}}\cr
        \hline
        AdaIN & \textbf{10.734} & \textbf{8.975} & \textbf{265.036} & 0.626 & \textbf{10.671**} & 8.453 & \textbf{246.736} & 0.710 & \textbf{10.746**} & 6.717 & \textbf{255.062} & \textbf{0.696**} \cr
        CycleGAN & 14.670 & 10.850 & 272.652 & 0.633 & 14.160 & 10.046 & 247.468 & \textbf{0.721} & 13.453 & 9.878 & 263.348 & 0.689 \cr
        StarGAN & 13.453 & 9.878 & 263.348 & \textbf{0.689**} & 17.920 & \textbf{1.310*} & 399.215 & 0.712 & 18.467 & \textbf{1.477} & 412.430 & 0.687 \cr
        \hline 
        \multicolumn{13}{c}{\bf{Custom Dataset}}\cr
        \hline
        AdaIN & \textbf{10.507*} & 6.639 & \textbf{195.487**} & \textbf{0.654} & 13.435 & 4.974 & \textbf{177.581*} &\textbf{0.737*} & \textbf{11.472} & 5.156 & \textbf{197.560**} & \textbf{0.693} \cr
        CycleGAN & 13.435 & 7.137 & 200.299 & 0.635 & \textbf{12.456} & 6.047 & 190.658 & 0.711 & 12.962 & 7.825 & 200.920 & 0.678 \cr
        StarGAN & 19.302 & \textbf{1.340**} & 434.779 & 0.646 & 18.028 & \textbf{1.362} & 376.450 & 0.715 & 19.608 & \textbf{1.402**} & 380.034 & 0.683 \cr
        \hline
    \end{tabular}
    \end{center}
    \leavevmode
    \footnotesize
    * the best result overall. \\
    ** the best result for the style.
\end{table}

\begin{table}
    \setlength\tabcolsep{4pt}
    \vspace{0.2em}
    \caption{Performance comparison of Style Transfer measured by various metrics grouped by model; Perceptual Distance (PD), Inception score (IS), Fréchet Inception Distance (FID) and Learned Perceptual Image Patch Similarity (LPIPS).}
    \centering
    \footnotesize
    \label{tab:performance_style_transfer_by_model}
    \begin{tabular}{ l|cccc|cccc|cccc }
        \hline
        \multirow{2}{*}{\bf{Method}}&\multicolumn{4}{c|}{\bf{Baroque}}&\multicolumn{4}{c|}{\bf{Impressionism}}&\multicolumn{4}{c}{\bf{Renaissance}}\cr
        &\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}&\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}&\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}\cr
        \hline
        \multicolumn{13}{c}{\bf{AdaIN}}\cr
        \hline
        AST-IQAD Dataset & 10.734 & 8.975 & 265.036 & 0.626 & \textbf{10.671} & 8.453 & 246.736 & 0.710 & \textbf{10.746} & 6.717 & 255.062 & \textbf{0.696} \cr
        Custom Dataset & \textbf{10.507} & \textbf{6.639} & \textbf{195.487} & \textbf{0.654} & 13.435 & \textbf{4.974} & \textbf{177.581} & \textbf{0.737} & 11.472 & \textbf{5.156} & \textbf{197.560} & 0.693 \cr
        \hline
        \multicolumn{13}{c}{\bf{CycleGAN}}\cr
        \hline
        AST-IQAD Dataset & 14.670 & 10.850 & 272.652 & 0.633 & 14.160 & 10.046 & 247.468 & \textbf{0.721} & 13.453 & 9.878 & 263.348 & \textbf{0.689} \cr
        Custom Dataset & \textbf{13.435} & \textbf{7.137} & \textbf{200.299} & \textbf{0.635} & \textbf{12.456} & \textbf{6.047} & \textbf{190.658*} & 0.711 & \textbf{12.962} & \textbf{7.825} & \textbf{200.920} & 0.678 \cr
        \hline
        \multicolumn{13}{c}{\bf{StarGAN}}\cr
        \hline
        AST-IQAD Dataset & \textbf{13.453} & 9.878 & \textbf{263.348} & \textbf{0.689} & \textbf{17.920} & \textbf{1.310} & 399.215 & 0.712 & \textbf{18.467} & 1.477 & 412.430 & \textbf{0.687} \cr
        Custom Dataset & 19.302 & \textbf{1.340} & 434.779 & 0.646 & 18.028 & 1.362 & \textbf{376.450} & \textbf{0.715} & 19.608 & \textbf{1.402} & \textbf{380.034} & 0.683 \cr
        \hline
    \end{tabular}
\end{table}

\subsection{Discussion}
\label{sec:baseline_discussion_style_transfer}
While the images are clearly stylized to look vaguely like the style of an art movement, it cannot be said that they belong in the same domain as actual artworks.
The stylized images can still be useful to augment the COCO dataset as the question whether stylized images can increase the evaluation results is still a useful one to ask.
It is obvious that the used evaluation metrics for style transfer are not very helpful.
Theoretically, they make complete sense, but they do not at all give a good reading on the quality of the images.
The numbers vary greatly, but this variance cannot be seen in the qualitative evaluation.
StarGAN, which experienced modal collapse, was still able to score high for \gls{IS}.
Ironically, StarGAN, while not retaining the content, does have the better oil painting characteristics.
The identity image, as show in Figure. \ref{fig:style_transfer_stargan_identity_image}, looks like modern art.
So, somewhere, the model does approach some kind of human like abstraction, or at least, as seen in abstract art.
Perhaps, how artists make abstractions can be used as an inductive bias in future models.

\begin{figure}[h]
    \centering
	\subcaptionbox{Input Image}{%
		\includegraphics[height=2in]{style_transfer_stargan_source_image}%
	}
	\subcaptionbox{Identity Image}{%
		\includegraphics[height=2in]{style_transfer_stargan_identity_image}%
	}
	\subcaptionbox{Richter, Gerard, \it{Weiß}}{%
		\includegraphics[height=2in]{gerhard-richter-polish-painting-wooarts-27}%
	}
	\caption{
        An example of an image created by StarGAN that has oil painting qualities. A painting from Gerard Richter as comparison is shown.
    }
    \label{fig:style_transfer_stargan_identity_image}
\end{figure}

The question remains why the style transfer algorithms aren't able to make correct mappings between different styles.
A first observation was discussed in section \ref{sec:baseline_dataset_style_transfer}.
It's a mistake to consider an art movement as a style, as even within the different art movements and realistic photographs there's a big variation in styles.
There can be different lighting, different brush stroke, different camera filter, different lines and different form.
There are plenty of things that can vary to make a distinct style.
It should be considered whether some things categorized as content now should instead be considered part of the style, like clothes.
Whether clothes should be considered content or style can depend on which domains the mapping is searched for.
Clothes change dramatically between the different time periods and this is clearly visible when comparing the artworks with photographs.
In this context, they should be considered a style.
While, when mapping within the same time period, they can be considered content.
The same argument can be made for architecture.

When looking at the datasets that CycleGAN and StarGAN are trained on, it becomes clear that most success is made when the domain is extremely specific.
As seen in Figure \ref{fig:AFHQ}, all the images contain the subject in the center of the image without any other content.
The custom datasets for the training contain a much higher disparity.
Perhaps it would be useful to transform different patches where the content is very similar with high certainty at a time, and then combine those to create the transformed image.
This can potentially be done by training on a dataset of 3d models where a shader is applied to simulate a different art style.
Instead of having to manually label thousands of images, it is possible to have several 3d models act out different poses and render them with different shaders.
A network can then be trained to recognize when patches have similar content and apply the style when they do.
This will mean that when using an arbitrary style, it might not always find a high similarity and the style transfer will not benefit from this.
\\

\begin{figure}[h]
	\centering
    \includegraphics[width=0.15\textwidth]{flickr_cat_000018}
    \includegraphics[width=0.15\textwidth]{pixabay_cat_003899}
    \includegraphics[width=0.15\textwidth]{flickr_dog_000014}
    \includegraphics[width=0.15\textwidth]{pixabay_dog_002595}
    \includegraphics[width=0.15\textwidth]{flickr_wild_000082}
    \includegraphics[width=0.15\textwidth]{pixabay_wild_000681}
	\caption{The \gls{AFHQ} dataset consists of images that are close-ups of animals.}
	\label{fig:AFHQ}
\end{figure}