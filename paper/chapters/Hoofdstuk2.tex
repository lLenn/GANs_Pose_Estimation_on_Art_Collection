\chapter{Establishing a Baseline}
\label{chap:baseline}
This chapter will establish the baseline that will be used to compare our results with.
For this, 2 to 3 algorithms from both pose estimation and style transfer will be explored.
The motivation for the choices of the algorithms will be explained in full detail.
First, style transfer will be applied to the COCO dataset to then estimate any poses from it.
The results wil give an indication of how well pose estimation will work on art collections.
More recently, a new dataset has emerged which will be of great help, the Human-Art dataset \cite{Ju2023}, with which we can directly check the pose estimation without an intermediary step.

\section{Choice of Pose Estimation}
\label{sec:baseline_pose_estimation}
(dirty version)
There are a few choice that are evident, does it have code available and is it compatible with the chosen dataset.
Another is time of inference, how fast can it estimate the pose? This paper doesn't need real-time inference, but a algorithm can both be fast and accurate \cite{William2021}
Want to explore a diverse set of estimators. (bottom-up, top-down, ...)
(SWAHR explain why...) \cite{SWARH} Faster network according to surveys. Uses the popular network HRNet. (Bottom-Up)
(KAPAO explain why...) \cite{William2021} Claims to be both fast and accurate. (Single-stage; explain single stage in literature study. It means that it does away with the top-down/bottom-up paradigm which are two-stage models.)
(VitPose explain why...) \cite{vitpose} Uses transformers

\section{Choice of Style Transfer}
\label{sec:baseline_style_transfer}
(dirty version)
A similar criteria as or pose estimation applies: does it have code, time of transformation
Uniquely: does it have pretrained models for the styles we want?
Does it apply transformation? (U-GAT-IT) (We don't want transformation, but interesting for future research)
(CycleGAN ...) \cite{Zhu2017} Has the most pre-trained art models available
(UNIT or StarGANv2 ...) \cite{Liu2017} Latent-space but no pretrained artistic model, but can we initialize weights with other models to speed up training?
1 - photo, 2 - baroque, 3 - impressionism, 4 - renaissance
(AdaIn) 
Another possible way to speed up training is to focus the dataset on human poses.
Which is why image retrieval has been discussed previously.
This way we can extract have more specialized datasets from the existing datasets.
Even with the genre categorization it's still too broad.
This has become apparent when training U-GAT-IT (This was before I realized that this model also does content transformation)
(StyleGAN) Because I feel that the images to be trained on are not genre or artist dependent, because there is still a great variety among them.
I think that it should be possible to train a style transfer with only a few samples.
It was said to me that StyleGAN did this.
I also want to research diffusion.

StarGAN experiences mode collapse after 100,000 iterations. (iterations because their implementation doesn't use epochs)
\section{Choice of Dataset}
(dirty version)
CIBR works well when there's something very recognisable, like a tennis court.
It has come to my attention that making a distinction between real life and art for style transfer is a mistake.
Viewing only art as having styles is a mistake.
Real life can have just as many different style.
Whether it is style of clothing, lighting or camera filter, while at the same time being possibly content.
The natural day and night cycle should be considered content, but artificial light should be considered style.
\subsection{Finding a good query image}
Several import aspects should be considered when searching for a good query image.
Does it contain the right kind of content and no other content to distract, like a car in the background or even a small flower in the foreground.
Should it be an image from the style we're trying to transform to?
The only images that yielded similar persons with different poses were when we queried an image with sports, like tennis.
Maybe an "instance image retrieval" algorithm is not the right algorithm? We could maybe get better results with a "category retrieval algorithm"?

I removed masking from the COCO dataset which was a mistake. 
When training a top-down estimator, then how does it make a destinction between people in a bounding box that are in the background?
How many people will it find that way?

\section{Pose Estimation after Applying Style Transfer to the COCO Dataset}
\label{sec:baseline_coco_style_transfer}

\subsection{Architecture}
\label{sec:baseline_coco_style_transfer_architecture}
Needed to implement a script that transforms a training model to a test model because CycleGAN leaves out certain layers for test, like dropout.

\subsection{Results}
How good is this as a measurement of the pose estimator?
It's entirely reliable on the style transfer.
For the CycleGAN model, there are some photo's that change little after applying style transfer.
For SWAHR, forgot to add masks in dataset (removed them while coping because didn't think it relevant, but there are images with multiple people where some people are not in the keypoint instances, so they need to be removed)
Only found out about this when a masked image was shown in the experiments
For ViTPose or other top-down pose estimators, the cropping of the image removes a lot of information that could potentially be relevant, like sitting on the back of an elephant or perspective.
Can a network even look for perspective and environmental clues in the first place?
Perhaps 2d is limited in that sense?
We consider the tests on 2 datasets: 
1. A set specifically made to measure style transfer.\cite{Chen2023} \cite{ioannou2024} Which contains a range of images over a different range of "measurements" 
Our own dataset of people ...
2. Custom dataset is create by running CIRQ on coco dataset with following query images: First query gives good results, but are dominated by outdoor images.
The following queries were done to find more images in a different context
While in the original paper there is only one style image used to train the network, here we compare the used content image and style image used for each transfer.
For CycleGAN and StarGAN, we cycle through the real images to determine the style loss.
\begin{table*}
    \setlength\tabcolsep{4pt}
    \vspace{0.2em}
    \caption{Performance comparison of Style Transfer measured by various metrics; Perceptual Distance (PD), Inception score (IS), Fr√©chet Inception Distance (FID) and Learned Perceptual Image Patch Similarity (LPIPS). }
    \centering
    \footnotesize
    \label{tab:performance_style_transfer}
    \begin{tabular}{ l|cccc|cccc|cccc }
        \hline
        \multirow{2}{*}{\bf{Method}}&\multicolumn{4}{c|}{\bf{Baroque}}&\multicolumn{4}{c|}{\bf{Renaissance}}&\multicolumn{4}{c}{\bf{Impressionism}}\cr
        &\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}&\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}&\bf{PD}&\bf{IS}&\bf{FID}&\bf{LPIPS}\cr
        \hline
        \multicolumn{13}{c}{\bf{AST-IQAD Database}}\cr
        \hline
        CycleGAN & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        AdaIN & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        StarGAN & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline 
        \multicolumn{13}{c}{\bf{Custom Database}}\cr
        \hline
        CycleGAN & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        AdaIN & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        StarGAN & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline 
    \end{tabular}
\end{table*}

\label{sec:baseline_coco_style_transfer_results}

\section{Pose Estimation on the Human-Art Dataset}
\label{sec:baseline_human_art}

\subsection{Architecture}
\label{sec:baseline_human_art_architecture}

\subsection{Results}
\label{sec:baseline_human_art_results}

SWAHR and HumanArt Dataset (validation set w32\_512)\\
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.469\\
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.688\\
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.499\\
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.066\\
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.512\\
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.529\\
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.726\\
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.562\\
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.111\\
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.573\\
| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\\
|---|---|---|---|---|---|---|---|---|---|---|\\
| SWAHR | 0.469 | 0.688 | 0.499 | 0.066 | 0.512 | 0.529 | 0.726 | 0.562 | 0.111 | 0.573 |\\

SWAHR and HumanArt Dataset (validation set w48\_640)\\
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.494\\
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.705\\
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.526\\
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.083\\
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.538\\
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.556\\
Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.749\\
Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.592\\
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.149\\
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.600\\
| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\\
|---|---|---|---|---|---|---|---|---|---|---|\\
| SWAHR | 0.494 | 0.705 | 0.526 | 0.083 | 0.538 | 0.556 | 0.749 | 0.592 | 0.149 | 0.600 |\\

\begin{table*}
    \setlength\tabcolsep{4pt}
    \caption{
        Establishing a baseline for Pose Estimation on Artworks; measuring Percentage of Correct Keypoints (PCK) and Average Precision/Recall (AP/AR).
        The first section shows the performance of the plain models on The COCO dataset measured.
        The second shows the performance of the different models on the Human-Art dataset.
    }
    \centering
    \footnotesize
    \label{tab:baseline_pose_estimation_after_style_transfer}
    \begin{tabular}{ l|cc|ccccc|ccccc }
        \hline
        \bf{Method}&\bf{PCK@0.2}&\bf{PCKh@0.5}&\bf{AP}&\bf{AP$^{50}$}&\bf{AP$^{75}$}&\bf{AP$^{M}$}&\bf{AP$^{L}$}&\bf{AR}&\bf{AR$^{50}$}&\bf{AR$^{75}$}&\bf{AR$^{M}$}&\bf{AR$^{L}$}\cr
        \hline
        \multicolumn{13}{c}{\bf{COCO dataset}}\cr
        \multicolumn{13}{c}{\bf{Trained on COCO}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Human-Art Dataset}}\cr
        \multicolumn{13}{c}{\bf{Trained on COCO}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on COCO + Mixed Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on COCO + Impressionism Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on Mixed Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on Impressionism Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
    \end{tabular}
\end{table*}


\begin{table*}
    \setlength\tabcolsep{4pt}
    \caption{
        Establishing a baseline for Pose Estimation on Artworks; measuring Percentage of Correct Keypoints (PCK) and Average Precision/Recall (AP/AR).
        The COCO dataset is transformed with various Style Transfer models on which performance is measured. }
    \centering
    \footnotesize
    \label{tab:baseline_pose_estimation_after_style_transfer}
    \begin{tabular}{ l|cc|ccccc|ccccc }
        \hline
        \bf{Method}&\bf{PCK@0.2}&\bf{PCKh@0.5}&\bf{AP}&\bf{AP$^{50}$}&\bf{AP$^{75}$}&\bf{AP$^{M}$}&\bf{AP$^{L}$}&\bf{AR}&\bf{AR$^{50}$}&\bf{AR$^{75}$}&\bf{AR$^{M}$}&\bf{AR$^{L}$}\cr
        \hline
        \multicolumn{13}{c}{\bf{CycleGAN}}\cr
        \multicolumn{13}{c}{\bf{Trained on COCO + Mixed Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on COCO + Impressionism Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on Mixed Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on Impressionism Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{AdaIN}}\cr
        \multicolumn{13}{c}{\bf{Trained on COCO + Mixed Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on COCO + Impressionism Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on Mixed Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
        \multicolumn{13}{c}{\bf{Trained on Impressionism Style Transfer}}\cr
        \hline
        SWAHR & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        ViTPose & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \cr
        \hline
    \end{tabular}
\end{table*}

\section{Related Papers}
Enhancing Human Pose Estimation in Ancient Vase Paintings via Perceptually-grounded Style Transfer Learning \cite{Madhu2020}\\
\subsection{Results}
\label{sec:baseline_related_papers_results}
Compare results with related paper

Discuss the code and discussions during implementation.
Also, what could be done differently (own implementation)
Discuss how there are many different ways to "choose" the style (change model (cyclegan), choose number (stargan), use style image)
This could be solved by creating a new interface for the styles with each their own options, etc 
Make everything highly configurable

With all style transfer, I notice that when trying to convert to photo, they always seem to confuse foreground and background.
You can tell from the stargan training that this seems to be solely with photo.
This could be because of my dataset, but it can also be that paintings aren't as high contrast and lines between foreground and background are less vage.
(check paper about making distinction between foreground and background)

The dataset could have more images, but just not more paintings, but also more different faces and angles of the body, etc, positioned all over the image.
Also examine how the perception field works, like how does it actually work?

Style transfer encoders lose data, one should wonder then whether this is actually a good approach.
Should there instead be more information "encoded" then is visible on the image.
The network should basically be able to render the "content" in 3d.

\section{Discussion}
\label{sec:baseline_discussion}
Already it is apparent that pose estimation on art collections is strongly dependent on the efficacy of the style transfer.
Use something else than visdom, because it takes too long to save when there are too many datapoints.
