\chapter{Establishing a Baseline}
\label{chap:baseline}
This chapter will establish the baseline that will be used to compare our results with.
For this, 2 to 3 algorithms from both pose estimation and style transfer will be explored.
The motivation for the choices of the algorithms will be explained in full detail.
First, style transfer will be applied to the COCO dataset to then estimate any poses from it.
The results wil give an indication of how well pose estimation will work on art collections.
More recently, a new dataset has emerged which will be of great help, the Human-Art dataset \cite{Ju2023}, with which we can directly check the pose estimation without an intermediary step.

\section{Choice of Pose Estimation}
\label{sec:baseline_pose_estimation}
(dirty version)
There are a few choice that are evident, does it have code available and is it compatible with the chosen dataset.
Another is time of inference, how fast can it estimate the pose? This paper doesn't need real-time inference, but a algortihm can both be fast and accurate \cite{William2021}
Want to explore a diverse set of estimators. (bottom-up, top-down, ...)
(SWAHR explain why...) \cite{SWARH} Faster network according to surveys. Uses the popular network HRNet. (Bottom-Up)
(KAPAO explain why...) \cite{William2021} Claims to be both fast and accurate. (Single-stage; explain single stage in literature study. It means that it does away with the top-down/bottom-up paradigm which are two-stage models.)
(VitPose explain why...) \cite{vitpose} Uses transformers

\section{Choice of Style Transfer}
\label{sec:baseline_style_transfer}
(dirty version)
A similar criteria as or pose estimation applies: does it have code, time of transformation
Uniquely: does it have pretrained models for the styles we want?
Does it apply transformation? (U-GAT-IT) (We don't want transformation, but interesting for future research)
(CycleGAN ...) \cite{Zhu2017} Has the most pre-trained art models available
(UNIT or StarGANv2 ...) \cite{Liu2017} Latent-space but no pretrained artistic model, but can we initialize weights with other models to speed up training?
(AdaIn) 
Another possible way to speed up training is to focus the dataset on human poses.
Which is why image retrieval has been discussed previously.
This way we can extract have more specialized datasets from the existing datasets.
Even with the genre categorization it's still too broad.
This has become apparent when training U-GAT-IT (This was before I realized that this model also does content transformation)

\section{Choice of Dataset}
(dirty version)
CIBR works well when there's something very recognisable, like a tennis court.
It has come to my attention that making a distinction between real life and art for style transfer is a mistake.
Viewing only art as having styles is a mistake.
Real life can have just as many different style.
Whether it is style of clothing, lighting or camera filter, while at the same time being possibly content.
The natural day and night cycle should be considered content, but artificial light should be considered style.
\subsection{Finding a good query image}
Several import aspects should be considered when searching for a good query image.
Does it contain the right kind of content and no other content to distract, like a car in the background or even a small flower in the foreground.
Should it be an image from the style we're trying to transform to?
The only images that yielded similar persons with different poses were when we queried an image with sports, like tennis.
Maybe an "instance image retrieval" algorithm is not the right algorithm? We could maybe get better results with a "category retrieval algorithm"?

\section{Pose Estimation after Applying Style Transfer to the COCO Dataset}
\label{sec:baseline_coco_style_transfer}

\subsection{Architecture}
\label{sec:baseline_coco_style_transfer_architecture}
Needed to implement a script that transforms a training model to a test model because CycleGAN leaves out certain layers for test, like dropout.

\subsection{Results}
How good is this as a measurement of the pose estimator?
It's entirely reliable on the style transfer.
For the CycleGAN model, there are some photo's that change little after applying style transfer.

\label{sec:baseline_coco_style_transfer_results}

\section{Pose Estimation on the Human-Art Dataset}
\label{sec:baseline_human_art}

\subsection{Architecture}
\label{sec:baseline_human_art_architecture}

\subsection{Results}
\label{sec:baseline_human_art_results}

SWAHR and HumanArt Dataset (validation set w32\_512)\\
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.469\\
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.688\\
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.499\\
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.066\\
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.512\\
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.529\\
 Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.726\\
 Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.562\\
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.111\\
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.573\\
| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\\
|---|---|---|---|---|---|---|---|---|---|---|\\
| SWAHR | 0.469 | 0.688 | 0.499 | 0.066 | 0.512 | 0.529 | 0.726 | 0.562 | 0.111 | 0.573 |\\

SWAHR and HumanArt Dataset (validation set w48\_640)\\
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.494\\
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.705\\
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.526\\
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.083\\
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.538\\
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.556\\
Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.749\\
Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.592\\
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.149\\
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.600\\
| Arch | AP | Ap .5 | AP .75 | AP (M) | AP (L) | AR | AR .5 | AR .75 | AR (M) | AR (L) |\\
|---|---|---|---|---|---|---|---|---|---|---|\\
| SWAHR | 0.494 | 0.705 | 0.526 | 0.083 | 0.538 | 0.556 | 0.749 | 0.592 | 0.149 | 0.600 |\\

\section{Related Papers}
Enhancing Human Pose Estimation in Ancient Vase Paintings via Perceptually-grounded Style Transfer Learning \cite{Madhu2020}\\
\subsection{Results}
\label{sec:baseline_related_papers_results}
Compare results with related paper

Discuss the code and discussions during implementation.
Also, what could be done differently (own implementation)
Discuss how there are many different ways to "choose" the style (change model (cyclegan), choose number (stargan), use style image)
This could be solved by creating a new interface for the styles with each their own options, etc 
Make everything highly configurable

With all style transfer, I notice that when trying to convert to photo, they always seem to confuse foreground and background.
You can tell from the stargan training that this seems to be solely with photo.
This could be because of my dataset, but it can also be that paintings aren't as high contrast and lines between foreground and background are less vage.
(check paper about making distinction between foreground and background)

The dataset could have more images, but just not more paintings, but also more different faces and angles of the body, etc, positioned all over the image.
Also examine how the perception field works, like how does it actually work?

\section{Discussion}
\label{sec:baseline_discussion}
Already it is apparent that pose estimation on art collections is strongly dependent on the efficacy of the style transfer.
