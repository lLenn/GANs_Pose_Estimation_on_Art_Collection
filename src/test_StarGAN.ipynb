{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7457bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n",
      "env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING 1\n",
    "%env PYTORCH_CUDA_ALLOC_CONF max_split_size_mb:512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, cv2, torch\n",
    "sys.path.append(f\"{os.getcwd()}\")\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "from style_transfer.networks import StarGAN, StarGANConfig\n",
    "from StarGAN.core.data_loader import get_train_loader, get_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709ebb01-15a7-4c7d-bc50-093ca2082cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config = StarGANConfig.create(\"style_transfer/config/stargan.yaml\")\n",
    "    # Dataset\n",
    "    config.defrost()\n",
    "    config.train_img_dir = \"../../../Datasets/custom/StarGANTrainingSmall/train\"\n",
    "    config.val_img_dir = \"../../../Datasets/custom/StarGANTrainingSmall/val\"\n",
    "    config.src_dir = \"../../../Datasets/custom/StarGANTrainingSmall\"\n",
    "    config.checkpoint_dir = \"../../../Models/stargan\"\n",
    "    config.result_dir = \".output/results\"\n",
    "    config.eval_dir = \".output/results\"\n",
    "    config.num_domains = 4\n",
    "    # Training\n",
    "    config.mode = \"train\"\n",
    "    config.num_workers = 4\n",
    "    config.total_iters = 5\n",
    "    config.batch_size = 1\n",
    "    config.val_batch_size = 1\n",
    "    config.print_every = 1\n",
    "    config.sample_every = 1\n",
    "    config.save_every = 1\n",
    "    config.eval_every = 10000\n",
    "    config.num_outs_per_domain = 1\n",
    "    config.continue_training = False\n",
    "    \n",
    "    # Visdom\n",
    "    config.name = \"test_stargan\"\n",
    "    config.display_server = \"http://localhost\"\n",
    "    config.display_port = 8097\n",
    "    config.display_env = \"test_stargan\"\n",
    "    config.freeze()\n",
    "    \n",
    "    network = StarGAN(config)\n",
    "    network.train(\n",
    "        dataloader_src=get_train_loader(\n",
    "            root=config.train_img_dir,\n",
    "            which='source',\n",
    "            img_size=config.img_size,\n",
    "            batch_size=config.batch_size,\n",
    "            prob=config.randcrop_prob,\n",
    "            num_workers=config.num_workers),\n",
    "        dataloader_ref=get_train_loader(\n",
    "            root=config.train_img_dir,\n",
    "            which='reference',\n",
    "            img_size=config.img_size,\n",
    "            batch_size=config.batch_size,\n",
    "            prob=config.randcrop_prob,\n",
    "            num_workers=config.num_workers),\n",
    "        dataloader_val=get_test_loader(\n",
    "            root=config.val_img_dir,\n",
    "            img_size=config.img_size,\n",
    "            batch_size=config.val_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=config.num_workers)\n",
    "    )\n",
    "\n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(imagePath, direction=\"AtoB\"):\n",
    "    config = StarGANConfig.create(\"style_transfer/config/stargan.yaml\")\n",
    "    network = StarGAN(config)\n",
    "    network.loadModel(\"../../../Models/afhq\", 100000)\n",
    "    with open(imagePath, 'rb') as file:\n",
    "        image = Image.open(file)\n",
    "        image.convert(\"RGB\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    image = torch.stack((transform(image),)).to(\"cuda\")\n",
    "\n",
    "    if direction == \"AtoB\":\n",
    "        style = torch.tensor([1]).to(\"cuda\")\n",
    "        image = network.imageToStyle(image, style)\n",
    "    elif direction == \"BtoA\":\n",
    "        style = torch.tensor([0]).to(\"cuda\")\n",
    "        image = network.imageToStyle(image, style)\n",
    "    \n",
    "    image = image[0] * 0.5 + 0.5\n",
    "    image = image.detach().cpu().numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(os.path.join(\"../.output/results/\", \"test_StarGAN.png\"), image * 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cad5b97-7589-4fd8-90de-ea55cd9a778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of generator: 33892995\n",
      "Number of parameters of mapping_network: 4079872\n",
      "Number of parameters of style_encoder: 20982592\n",
      "Number of parameters of discriminator: 20853316\n",
      "Initializing generator...\n",
      "Initializing mapping_network...\n",
      "Initializing style_encoder...\n",
      "Initializing discriminator...\n",
      "Preparing DataLoader to fetch source images during the training phase...\n",
      "Preparing DataLoader to fetch reference images during the training phase...\n",
      "Preparing DataLoader for the generation phase...\n",
      "Start training...\n",
      "(iters: 1, time: 7.198) D/latent_real: 0.579 D/latent_fake: 0.307 D/latent_reg: 0.001 D/ref_real: 0.000 D/ref_fake: 0.000 D/ref_reg: 0.004 G/latent_adv: 25.414 G/latent_sty: 1.617 G/latent_ds: 0.769 G/latent_cyc: 0.901 G/ref_adv: 17.313 G/ref_sty: 1.033 G/ref_ds: 0.286 G/ref_cyc: 0.879 G/lambda_ds: 1.000 \n",
      "(iters: 2, time: 10.462) D/latent_real: 0.715 D/latent_fake: 8.404 D/latent_reg: 0.001 D/ref_real: 0.000 D/ref_fake: 0.000 D/ref_reg: 0.002 G/latent_adv: 27.986 G/latent_sty: 1.680 G/latent_ds: 0.752 G/latent_cyc: 0.946 G/ref_adv: 18.731 G/ref_sty: 1.005 G/ref_ds: 0.773 G/ref_cyc: 0.889 G/lambda_ds: 1.000 \n",
      "(iters: 3, time: 14.149) D/latent_real: 6.528 D/latent_fake: 0.000 D/latent_reg: 0.001 D/ref_real: 0.000 D/ref_fake: 0.003 D/ref_reg: 0.003 G/latent_adv: 6.209 G/latent_sty: 1.378 G/latent_ds: 0.759 G/latent_cyc: 1.081 G/ref_adv: 8.036 G/ref_sty: 1.014 G/ref_ds: 0.499 G/ref_cyc: 1.049 G/lambda_ds: 1.000 \n",
      "(iters: 4, time: 17.483) D/latent_real: 2.282 D/latent_fake: 4.858 D/latent_reg: 0.001 D/ref_real: 0.000 D/ref_fake: 0.000 D/ref_reg: 0.004 G/latent_adv: 22.881 G/latent_sty: 1.560 G/latent_ds: 0.837 G/latent_cyc: 1.104 G/ref_adv: 8.488 G/ref_sty: 0.685 G/ref_ds: 0.509 G/ref_cyc: 1.006 G/lambda_ds: 1.000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# transform(\"../.output/results/test_StarGAN copy 2.png\", \"BtoA\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m config\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[0;32m     32\u001b[0m network \u001b[38;5;241m=\u001b[39m StarGAN(config)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_train_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_img_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandcrop_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_train_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_img_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandcrop_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_test_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_img_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\UGent\\Master Industriele Wetenschappen\\Masterproef\\Repositories\\GANs_Pose_Estimation_on_Art_Collection\\src\\style_transfer\\networks\\StarGAN.py:121\u001b[0m, in \u001b[0;36mStarGAN.train\u001b[1;34m(self, dataloader_src, dataloader_ref, dataloader_val)\u001b[0m\n\u001b[0;32m    119\u001b[0m d_loss, d_losses_latent \u001b[38;5;241m=\u001b[39m compute_d_loss(nets, args, x_real, y_org, y_trg, z_trg\u001b[38;5;241m=\u001b[39mz_trg)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_reset_grad()\n\u001b[1;32m--> 121\u001b[0m \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m optims\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    124\u001b[0m d_loss, d_losses_ref \u001b[38;5;241m=\u001b[39m compute_d_loss(nets, args, x_real, y_org, y_trg, x_ref\u001b[38;5;241m=\u001b[39mx_ref)\n",
      "File \u001b[1;32md:\\UGent\\Master Industriele Wetenschappen\\Masterproef\\Repositories\\GANs_Pose_Estimation_on_Art_Collection\\.venv_win\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\UGent\\Master Industriele Wetenschappen\\Masterproef\\Repositories\\GANs_Pose_Estimation_on_Art_Collection\\.venv_win\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()\n",
    "# transform(\"../.output/results/test_StarGAN copy 2.png\", \"BtoA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
